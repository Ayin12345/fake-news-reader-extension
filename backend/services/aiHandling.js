// AI handling services for backend
// This mirrors the functionality from src/utils/aiHandling.ts but runs server-side

import { createAPIError, createNetworkError, createTimeoutError, createConfigurationError, retryOperation, ErrorCode, isRetryableError } from '../utils/errors.js';

export async function fetchOpenAI(content, apiKey) {
  if (!apiKey) {
    throw createConfigurationError(
      ErrorCode.MISSING_API_KEY,
      'OpenAI API key not configured',
      { provider: 'OpenAI' }
    );
  }

  return retryOperation(async () => {
    console.time('[Backend AI] OpenAI request');
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [{ role: 'user', content }]
      })
    });
    console.timeEnd('[Backend AI] OpenAI request');

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({ error: { message: 'Unknown error' } }));
      const status = response.status;
      
      // Determine error type and retryability
      let error;
      if (status === 401 || status === 403) {
        // Authentication/authorization errors are not retryable
        error = createAPIError(
          ErrorCode.API_KEY_INVALID,
          errorData.error?.message || `OpenAI API error: ${status}`,
          { status, provider: 'OpenAI', errorData },
          false
        );
      } else if (status === 429) {
        // Rate limit errors are retryable
        error = createAPIError(
          ErrorCode.API_RATE_LIMITED,
          errorData.error?.message || `OpenAI API rate limit exceeded`,
          { status, provider: 'OpenAI', errorData },
          true
        );
      } else if (status >= 500) {
        // Server errors are retryable
        error = createAPIError(
          ErrorCode.OPENAI_ERROR,
          errorData.error?.message || `OpenAI API error: ${status}`,
          { status, provider: 'OpenAI', errorData },
          true
        );
      } else {
        // Other errors are generally not retryable
        error = createAPIError(
          ErrorCode.OPENAI_ERROR,
          errorData.error?.message || `OpenAI API error: ${status}`,
          { status, provider: 'OpenAI', errorData },
          false
        );
      }
      
      throw error;
    }

    const data = await response.json();
    if (data.choices && data.choices[0] && data.choices[0].message.content) {
      return data.choices[0].message.content;
    } else {
      throw createAPIError(
        ErrorCode.INVALID_RESPONSE_FORMAT,
        data.error?.message || 'No response from OpenAI',
        { provider: 'OpenAI', responseData: data },
        false
      );
    }
  }, 3, 1000, 'OpenAI');
}

async function fetchGeminiWithModel(content, apiKey, model) {
  if (!apiKey) {
    throw createConfigurationError(
      ErrorCode.MISSING_API_KEY,
      'Gemini API key not configured',
      { provider: 'Gemini', model }
    );
  }

  return retryOperation(async () => {
    console.time(`[Backend AI] Gemini ${model} request`);
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          contents: [{
            parts: [{
              text: content
            }]
          }],
          generationConfig: {
            temperature: 0.7,
            maxOutputTokens: 6000
          }
        })
      }
    );
    console.timeEnd(`[Backend AI] Gemini ${model} request`);

    if (!response.ok) {
      const errorText = await response.text();
      const status = response.status;
      
      let error;
      if (status === 401 || status === 403) {
        error = createAPIError(
          ErrorCode.API_KEY_INVALID,
          `Gemini ${model} API authentication error: ${status}`,
          { status, provider: 'Gemini', model, errorText },
          false
        );
      } else if (status === 429) {
        error = createAPIError(
          ErrorCode.API_RATE_LIMITED,
          `Gemini ${model} API rate limit exceeded`,
          { status, provider: 'Gemini', model, errorText },
          true
        );
      } else if (status >= 500) {
        error = createAPIError(
          ErrorCode.GEMINI_ERROR,
          `Gemini ${model} API error: ${status} ${response.statusText}`,
          { status, provider: 'Gemini', model, errorText },
          true
        );
      } else {
        error = createAPIError(
          ErrorCode.GEMINI_ERROR,
          `Gemini ${model} API error: ${status} ${response.statusText} - ${errorText}`,
          { status, provider: 'Gemini', model, errorText },
          false
        );
      }
      
      throw error;
    }

    const data = await response.json();
    
    if (data.candidates && data.candidates[0]) {
      const candidate = data.candidates[0];
      
      if (candidate.finishReason === 'MAX_TOKENS') {
        throw createAPIError(
          ErrorCode.INVALID_RESPONSE_FORMAT,
          `Gemini ${model} response was truncated due to token limit.`,
          { provider: 'Gemini', model, finishReason: candidate.finishReason },
          false
        );
      }
      
      if (candidate.finishReason === 'SAFETY') {
        throw createAPIError(
          ErrorCode.INVALID_RESPONSE_FORMAT,
          `Gemini ${model} response was blocked due to safety filters.`,
          { provider: 'Gemini', model, finishReason: candidate.finishReason },
          false
        );
      }
      
      if (candidate.content && candidate.content.parts && candidate.content.parts[0] && candidate.content.parts[0].text) {
        return candidate.content.parts[0].text;
      }
      
      if (candidate.content && candidate.content.text) {
        return candidate.content.text;
      }
      
      throw createAPIError(
        ErrorCode.INVALID_RESPONSE_FORMAT,
        `Gemini ${model} response incomplete. Finish reason: ${candidate.finishReason || 'unknown'}`,
        { provider: 'Gemini', model, finishReason: candidate.finishReason },
        false
      );
    } else {
      throw createAPIError(
        ErrorCode.INVALID_RESPONSE_FORMAT,
        data.error?.message || `No candidates in Gemini ${model} response`,
        { provider: 'Gemini', model, responseData: data },
        false
      );
    }
  }, 3, 1000, `Gemini ${model}`);
}

export async function fetchGemini(content, apiKey) {
  // Try Gemini 2.5 Flash first (faster, newer)
  try {
    return await fetchGeminiWithModel(content, apiKey, 'gemini-2.5-flash');
  } catch (error) {
    console.warn('[Backend AI] Gemini 2.5 Flash failed, trying backup model');
    
    // Fallback to Gemini 2.5 Flash Lite
    try {
      return await fetchGeminiWithModel(content, apiKey, 'gemini-2.5-flash-lite');
    } catch (backupError) {
      console.error('[Backend AI] Both Gemini models failed:', backupError);
      // Re-throw the original error if backup also fails
      throw createAPIError(
        ErrorCode.GEMINI_ERROR,
        'Analysis failed due to AI model limitations. Please try again later.',
        { 
          originalError: error.message,
          backupError: backupError.message,
          provider: 'Gemini'
        },
        // Only retryable if both errors are retryable
        isRetryableError(error) && isRetryableError(backupError)
      );
    }
  }
}

